services:
  # Dev: API with hot reload. Source mounted, code changes restart uvicorn.
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    image: ai-platform-api
    ports:
      - "8000:8000"
    volumes:
      - ./api:/app
    command: ["sh", "-c", "alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"]
    environment:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@db:5432/aiplatform
      REDIS_URL: redis://redis:6379/0
      LLM_PROVIDER: ollama
      LLM_BASE_URL: http://ollama:11434
      LLM_MODEL: llama3.2:1b
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
      ollama:
        condition: service_started

  # Dev: Frontend Vite server with hot reload. Source mounted.
  frontend:
    image: node:22-alpine
    working_dir: /app
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      # Use host.docker.internal so frontend reaches API via host's port 8000
      # (avoids ENOTFOUND when api container restarts or is temporarily down)
      VITE_PROXY_TARGET: http://host.docker.internal:8000
    command: sh -c "npm ci && npm run dev -- --host 0.0.0.0"
    depends_on:
      api:
        condition: service_started

  migrate:
    build:
      context: ./api
      dockerfile: Dockerfile
    image: ai-platform-api
    command: alembic upgrade head
    environment:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@db:5432/aiplatform
    depends_on:
      db:
        condition: service_healthy

  # Optional: production-style frontend for local testing. Use K8s for real production.
  frontend-prod:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: ai-platform-ui
    ports:
      - "80:80"
    depends_on:
      api:
        condition: service_started
    profiles:
      - prod

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # Keep model loaded indefinitely to avoid 10-30s cold start on first request
      OLLAMA_KEEP_ALIVE: "-1"

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: aiplatform
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
  frontend_node_modules:
  ollama_data:
